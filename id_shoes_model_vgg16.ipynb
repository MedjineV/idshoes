{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removable-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir le chemin du dataset\n",
    "DATASET_FOLDER = \"/home/nathan/id_shoes_pictures/dataset/images_vgg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create folders to store images for classification \n",
    "# for data in [\"train\", \"val\", \"test\"]:\n",
    "#     for classe in range(21):\n",
    "#         path_folder = os.path.join(FOLDER_DIR, data, str(classe))\n",
    "#         os.makedirs(path_folder, exist_ok=True)\n",
    "#     for image in os.listdir(DATA_DIR+data):\n",
    "#         path_src = os.path.join(DATA_DIR, data, image)\n",
    "#         path_dest = os.path.join(FOLDER_DIR, data, image.split(\"_\")[0])\n",
    "#         shutil.copy(path_src, path_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "understood-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir un simple transformer pour convertir les images en tensor et aussi les normaliser\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# créer nos datasets train et val\n",
    "train_data = datasets.ImageFolder(DATASET_FOLDER+\"train\", data_transforms[\"train\"])\n",
    "val_data = datasets.ImageFolder(DATASET_FOLDER+\"val\", data_transforms[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader to feed our network with batchsize 20. For windows users, Olé :) , set num_workers to 0\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=True, num_workers=4)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=20, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instrumental-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer le modèle vgg16, freezer toutes les couches et remplacer la dernière couche mais avec des paramètres qui seront mis à jour\n",
    "net = models.vgg16(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.classifier[6] = nn.Linear(4096,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "corresponding-bookmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=21, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utiliser le gpu s'il y'en a\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser la cross entropy comme fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# utiliser la descente de gradient stochastique et mettre uniquement à jour les parametres de la couche\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step\n",
    "epochs = 50\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 0:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, (i+1)*20, running_loss))\n",
    "            \n",
    "        running_loss = 0.0\n",
    "        \n",
    "    # we evaluate our model precision on val_dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('Accuracy of the network on the %d val images: %d %%' % (total, acc))\n",
    "    \n",
    "    torch.save(net.state_dict(), \"/home/nathan/code/Cours Stefano Millaci/model_weights/id_shoes_vgg16_weights_last.pt\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        print(epoch+1)\n",
    "        torch.save(net.state_dict(), \"/home/nathan/code/Cours Stefano Millaci/model_weights/id_shoes_vgg16_weights_best.pt\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominant-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index de nos classes\n",
    "class_names = {   \n",
    "    \"0\":\"AQS100\",\n",
    "    \"1\":\"AQS120\",\n",
    "    \"2\":\"AQUADOTS\",\n",
    "    \"3\":\"AQUAFUN\",\n",
    "    \"4\":\"BOCAGE 1\",\n",
    "    \"5\":\"SAILING100\",\n",
    "    \"6\":\"TBS 1\",\n",
    "    \"7\":\"TBS 10\",\n",
    "    \"8\":\"TBS 11\",\n",
    "    \"9\":\"TBS 12\",\n",
    "    \"10\":\"TBS 13\",\n",
    "    \"11\":\"TBS 14\",\n",
    "    \"12\":\"TBS 15\",\n",
    "    \"13\":\"TBS 2\",\n",
    "    \"14\":\"TBS 3\",\n",
    "    \"15\":\"TBS 4\",\n",
    "    \"16\":\"TBS 5\",\n",
    "    \"17\":\"TBS 6\",\n",
    "    \"18\":\"TBS 7\",\n",
    "    \"19\":\"TBS 8\",\n",
    "    \"20\":\"TBS 9\"\n",
    "}\n",
    "\n",
    "#class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder les poids de notre modèles\n",
    "#!mkdir /content/vgg16_model_weigths\n",
    "torch.save(net.state_dict(), \"/content/vgg16_model_weigths/id_shoes_weights_5_epochs_kfold_5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "widespread-bhutan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pour charger les poids de notre modèle\n",
    "net.load_state_dict(torch.load(\"/home/nathan/code/Cours Stefano Millaci/model_weights/id_shoes_vgg16_weights_best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demonstrated-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir le dataset de test et le dataloader de test. Sur Windows mettre num_workers à 0\n",
    "test_data = datasets.ImageFolder(DATASET_FOLDER+\"test\", data_transforms[\"val\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, num_workers=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "statutory-paintball",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d59cf23f6172>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# code pour calculer la précision sur les données de test\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Afficher la précision\n",
    "        print('La précision est %d %%' % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code pour calculer la précision sur les données de test\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Afficher la précision\n",
    "    print('La précision est %d %%' % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vertical-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18.0237, 12.8471,  6.2282,  1.7952,  0.2237,  1.5533, -0.8606, -5.5846,\n",
       "         -5.4529, -0.5034, -2.8410,  2.7344,  2.4049,  3.1068, -6.5865, -5.1063,\n",
       "         -0.3282, -5.3743, -8.0533, -2.1314, -4.4783]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = Image.open(\"/home/nathan/id_shoes_pictures/dataset/images_vgg/test/AQS100/20210211_154538.jpg\")\n",
    "image = data_transforms[\"val\"](image).unsqueeze(0)\n",
    "image = image.to(device)\n",
    "net.eval()\n",
    "output = net(image)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "display(output, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names[predicted.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code pour calculer la précision sur chacune de nos classes (21 classes)\n",
    "for epoch in range(5,51,5):\n",
    "    net.load_state_dict(torch.load(\"/home/nathan/code/Cours Stefano Millaci/model_weights/id_shoes_weights_{}_epochs.pt\".format(epoch)))\n",
    "    class_correct = list(0. for i in range(21))\n",
    "    class_total = list(0. for i in range(21))\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            # calculer le nombre de prédictions correctes pour la taille de batch 12\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    print(\"Pour {} épochs\".format(epoch))\n",
    "    for i in range(21):\n",
    "        print('Accuracy of %5s : %2d %%' % (class_names[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code pour calculer la précision sur chacune de nos classes (21 classes)\n",
    "class_correct = list(0. for i in range(21))\n",
    "class_total = list(0. for i in range(21))\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        # calculer le nombre de prédictions correctes pour la taille de batch 12\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "            \n",
    "for i in range(21):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        class_names[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-bullet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
